---
title: "Lab 10 - Grading the professor, Pt. 1"
author: "Allison Li"
date: "04202025"
output: github_document
---

## Load Packages and Data

```{r load-packages, message=FALSE, warning=FALSE}
##install.packages("openintro")
library(tidyverse) 
library(tidymodels)
library(openintro)
```

## Exercise 1

```{r exercise1_code}
glimpse(evals)

##Distribution:
ggplot(evals, aes(x = score)) +
  geom_histogram(
    binwidth = 0.5, fill = "pink", color = "lightblue") +
  labs(title = "Distribution of Course Evaluation Scores", x = "Score", y = "Count")
```

This is a left skewed distribution, indicating that most students' average professor evaluation scores were relatively good (4-5, excellent). This is what I expected since I believe most professors are good at teaching classes, while very little students indicating neutral or negative attitudes towards their professions. 

## Exercise 2

```{r exercise2_code}

ggplot(evals, aes(x=bty_avg, y=score, color = "pink")) + 
  geom_point(alpha = .6) +
  labs(
    title = "Scatterplot of Professor Evaluation Score vs. Beauty Rating",
    x = "Average Beauty Rating",
    y = "Course Evaluation Score"
  )
```

According to the graph, most dotes clustered on lower average beauty rating and higher course evaluation scores. 

## Exercise 3

```{r geom jitter}
ggplot(evals, aes(x=bty_avg, y=score, color = "pink")) + 
  geom_jitter(alpha = .6) +
  labs(
    title = "Scatterplot of Professor Evaluation Score vs. Beauty Rating",
    x = "Average Beauty Rating",
    y = "Course Evaluation Score"
  )
```

This graph looks less 'organized'. This introduction of Jitter (https://ggplot2.tidyverse.org/reference/geom_jitter.html) here indicates that it is a convenient shortcut for geom_point that adds a small amount of random variation to the location of each point. It can handle the overplotting since it adds a small amount of random noise to data to spread out the dots so we can see clearly how many dots there are in the same places. Although we have adjusted the alpha level, we can see from this graph better that most dots are lying in around 4 for average beauty rating and 4-5 around course evaluation score.

## Exercise 4

```{r linear model}
m_bty <- lm(
  score ~ bty_avg,
  data = evals
)
summary(m_bty)
```
Looking at the summary of the model, the function should look like: score = 3.8803 + .0666 × bty_avg

## Exercise 5

```{r replot}
ggplot(evals, aes(x=bty_avg, y=score, color = "pink")) + 
  geom_jitter(alpha = .6) +
  geom_smooth(method = "lm", se = FALSE, color = "orange", size = 1) +
  labs(
    title = "Scatterplot of Professor Evaluation Score vs. Beauty Rating",
    x = "Average Beauty Rating",
    y = "Course Evaluation Score"
  )
```

I am not sure why turning off the shading, but shading represents the 95% confidence interval for the regression line. I speculate that taking that off because it is not our focus for the visualization and it might distract people.

## Exercise 6

the slope suggests that higher average beauty rating of the professor is related with / can predict (?) slightly higher course evaluation scores. 

## Exercise 7

the intercept represents the according course evaluation scores for a professor who has a beauty rating of 0. I do not think it has any meaning in this context since no one has rated the beauty rating as 0 so the according course evaluation scores do not mean anything. 

## Exercise 8

the R2 value here is .04, indicating that the beauty rating explains only a small fraction (4%) of the variation in course evaluation rating scores. 

# Part 3
## Exercise 9

```{r gender lm}
m_gen <- lm(score ~ gender, data = evals)
summary(m_gen)

```

According to the tables, the reference level is female. The coefficients tell us that from female to male professors, there is a 0.14 increase in the course evaluation rating scores. More specifically, the average course evaluation score for female professors are 4.09 while 4.23 for male professors. 

## Exercise 10

For male: score = 4.0928 + .1415 × 1
For female: score = 4.0928 + .1415 × 0

## Exercise 11

```{r rank lm}
m_rank <- lm(score ~ rank, data = evals)
summary(m_rank)
```

According to the results, the reference level is teaching. The equation: score = 4.28431 − .12968 × tenure track − .14518 × tenured. This equation indicates that teaching professors are predicted to have an average score of 4.28. From teaching to tenure track professors, there is a .13 decrease in the average course evaluation rating scores. However, according to the p value, this predictor is not statistically significant, suggesting that the association is not meaningful. Additionally, from teaching to tenured professors, there is a .15 decrease in the average course evaluation rating scores. 

## Exercise 12

```{r reorder professor rank}
evals$rank_relevel <- relevel(evals$rank, ref = "tenure track") ##I asked GPT how to do this
```

## Exercise 13

```{r}
m_rank_relevel <- lm(score ~ rank_relevel, data = evals)
summary(m_rank_relevel)

unique(evals$rank_relevel) # to double check
```

According to the results, the reference level is tenured track The equation: score = 4.15463 + .12968 × teaching − .01550 × tenured. This equation indicates that tenure track professors are predicted to have an average course evaluation score of 4.15. From tenure track to teaching professors, there is a .13 increase in the average course evaluation rating scores. However, according to the p value, this predictor is not statistically significant, suggesting that the association is not meaningful. Additionally, from tenure track to tenured professors, there is a .02 decrease in the average course evaluation rating scores. This predictor is also not statsitically significant, indicating the prediction is not meaningful. In other words, there is no meaningful evidence that scores differ between tenured and tenure track professors. According to the R-squared value, the model explains only about 1.2% of the variation in evaluation scores based on professor's rank,indicating that professor rank is not a strong predictor of course evaluation scores.


## Exercise 14

```{r relabel rank}
evals <- evals %>%
  mutate(tenure_eligible = ifelse(rank %in% c("teaching"), "no", "yes"))
```

## Exercise 15

```{r tenure eligible}
m_tenure_eligible <- lm(score ~ tenure_eligible, data = evals)
summary(m_tenure_eligible)
```

According to the results, the reference level is tenured track non eligible (the teaching position). The equation: score = 4.2843 − .1406 × tenured eligible. This equation indicates that tenure track noneligible (teaching) professors are predicted to have an average course evaluation score of 4.28. From tenure non eligible track to eligible professors, there is a .14 decrease in the average course evaluation rating scores. This predicton is significant. According to the R-squared value, the model explains only about 1.1% of the variation in evaluation scores based on professor's rank, indicating that professor rank is not a strong predictor (but significant) of course evaluation scores.

